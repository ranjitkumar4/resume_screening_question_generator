# ðŸ¤– AI Resume Screening & Interview Question Generation Assistant

## Project Overview

This repository contains a **full-stack AI application** that helps HR and recruitment teams automate:

- Resume parsing
- Matching candidates to job descriptions
- Generating **personalized technical and behavioral interview questions**
- Providing **JD match scores** and summaries
- Maintaining **memory across evaluations**
- Evaluating the quality of generated questions via an **LLM evaluator agent**

The system uses **FastAPI** for backend APIs, **Streamlit** for a dynamic frontend, and **Groq LLM (`llama-3.3-70b-versatile`)** for AI reasoning.

---

## Folder Structure



---

## Setup Instructions

1. **Clone the repository**

```bash
git clone <repo_url>
cd resume_interview_agent

cp .env_example .env
# Edit .env and add your keys
GROQ_API_KEY="your_real_groq_api_key"
GROQ_MODEL_NAME="llama-3.3-70b-versatile"

pip install -r requirements.txt
uvicorn backend.main:app --reload
streamlit run frontend/streamlit_app.py

How to Use the App
Upload a resume (TXT format)
Upload a job description (TXT format)
Click Generate Results

View:
Parsed Resume Information
JD Match Score & Summary
Generated Technical & Behavioral Questions
LLM Evaluation / Critique
Stored Memory of evaluations

# resume_interview_agent/
# â”œâ”€â”€ backend/
# â”‚   â”œâ”€â”€ agents/                     # Modular agent scripts
# â”‚   â”‚   â”œâ”€â”€ resume_extractor_agent.py   # Parses resumes (FR-02)
# â”‚   â”‚   â”œâ”€â”€ jd_matcher_agent.py         # Matches resume vs JD (FR-04)
# â”‚   â”‚   â”œâ”€â”€ qgen_agent.py               # Generates technical & behavioral questions (FR-05/06)
# â”‚   â”‚   â”œâ”€â”€ evaluator_agent.py          # Evaluates agent outputs (manual/LLM/agent)
# â”‚   â”‚   â””â”€â”€ tool_rag_agent.py           # Optional tool / RAG integration (FR-08)
# â”‚   â”œâ”€â”€ core/                       # Core utilities used by agents
# â”‚   â”‚   â”œâ”€â”€ llm_client.py                # LLM API wrapper
# â”‚   â”‚   â”œâ”€â”€ file_utils.py                # Resume/JD file reading & parsing
# â”‚   â”‚   â”œâ”€â”€ embeddings_utils.py          # Semantic similarity & embeddings
# â”‚   â”‚   â”œâ”€â”€ prompts.py                   # Centralized prompts for LLM calls
# â”‚   â”‚   â””â”€â”€ memory_manager.py            # Context/memory management (FR-07)
# â”‚   â”œâ”€â”€ config/                     # FastAPI configuration & environment
# â”‚   â”‚   â””â”€â”€ settings.py                  # Pydantic BaseSettings, loads .env
# â”‚   â”œâ”€â”€ data/                       # Sample/resume, JD, memory storage
# â”‚   â”‚   â”œâ”€â”€ resumes/
# â”‚   â”‚   â”‚   â”œâ”€â”€ resume1.txt
# â”‚   â”‚   â”‚   â””â”€â”€ resume2.txt
# â”‚   â”‚   â”œâ”€â”€ jds/
# â”‚   â”‚   â”‚   â”œâ”€â”€ jd1.txt
# â”‚   â”‚   â”‚   â””â”€â”€ jd2.txt
# â”‚   â”‚   â””â”€â”€ memory_store.json            # Persistent or mock memory
# â”‚   â”œâ”€â”€ main.py                     # FastAPI backend entry point / agent orchestration
# â”‚   â””â”€â”€ tests/                      # Unit / integration tests for agents & utils
# â”œâ”€â”€ frontend/
# â”‚   â”œâ”€â”€ streamlit_app.py            # Main Streamlit UI
# â”‚   â”œâ”€â”€ pages/                      # Optional multipage UI for modular screens
# â”‚   â””â”€â”€ assets/                     # Optional images/icons for UI
# â”œâ”€â”€ evaluation_reports/             # Stores manual/LLM evaluation outputs (Markdown/CSV)
# â”œâ”€â”€ notebooks/                      # Optional notebooks for experimentation / analysis
# â”œâ”€â”€ logs/                           # Optional runtime logs, API calls, or memory changes
# â”œâ”€â”€ .env_example                    # Example environment variables (submit only this)
# â”œâ”€â”€ pyproject.toml                  # UV dependency management (replaces requirements.txt)
# â”œâ”€â”€ folder_structure.py             # Script to generate folders/files automatically
# â””â”€â”€ README.md                       # Setup, instructions, project documentation